{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ib-Programmer/computer_vision_expirement/blob/main/notebooks/Phase2_Image_Enhancement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjyLLtKP13Nw"
      },
      "source": [
        "# Phase 2: Image Enhancement Evaluation\n",
        "Evaluating Restormer, FFA-Net, and Zero-DCE++ on degraded outdoor images.\n",
        "\n",
        "**Metrics**: PSNR, SSIM, NIQE, Inference Latency\n",
        "**Goal**: Select the best enhancement model for the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TWhVymbG13Ny",
        "outputId": "7de35c56-c18f-496a-d0a5-2600369700c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content\n",
            "Cloning into 'computer_vision_expirement'...\n",
            "remote: Enumerating objects: 80, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 80 (delta 43), reused 51 (delta 21), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (80/80), 1.19 MiB | 9.48 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n",
            "/content/computer_vision_expirement\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.2/276.2 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m134.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.4/299.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.4/226.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.2.2 requires transformers<6.0.0,>=4.41.0, but you have transformers 4.37.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "--- Downloading datasets to local disk ---\n",
            "Phase 1: Dataset Download\n",
            "Base directory: /content/computer_vision_expirement\n",
            "Datasets directory: /content/computer_vision_expirement/datasets\n",
            "\n",
            "============================================================\n",
            "DOWNLOADING: RTTS (Real-world Task-driven Testing Set)\n",
            "============================================================\n",
            "  Trying Kaggle API (tuncnguyn/rtts-dataset)...\n",
            "  Kaggle method failed: Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n",
            "  Trying UT Austin Box mirror (official RESIDE-beta)...\n",
            "  Downloading: https://utexas.app.box.com/index.php?rm=box_download_shared_file&shared_name=2yekra41udg9rgyzi3ysi513cps621qz&file_id=f_766454923366\n",
            "RTTS: 1.07GB [00:43, 24.6MB/s]                \n",
            "  Saved to: /content/computer_vision_expirement/datasets/rtts/RTTS.zip\n",
            "  Extracting RTTS.zip...\n",
            "  Extracted to: /content/computer_vision_expirement/datasets/rtts\n",
            "  RTTS download complete!\n",
            "  Location: /content/computer_vision_expirement/datasets/rtts\n",
            "\n",
            "============================================================\n",
            "DOWNLOADING: LFW (Labeled Faces in the Wild)\n",
            "============================================================\n",
            "  Trying sklearn.datasets.fetch_lfw_people...\n",
            "  Downloaded via sklearn: 13233 images\n",
            "  LFW download complete!\n",
            "  Location: /content/computer_vision_expirement/datasets/lfw\n",
            "\n",
            "============================================================\n",
            "DOWNLOADING: WiderFace\n",
            "============================================================\n",
            "  Downloading WIDER_train.zip from Google Drive...\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=15hGDLhsx8bLgLcIRD5DhYt5iBxnjNF1M\n",
            "From (redirected): https://drive.google.com/uc?id=15hGDLhsx8bLgLcIRD5DhYt5iBxnjNF1M&confirm=t&uuid=1f6f8f48-3d74-4d74-b77b-af08e3050009\n",
            "To: /content/computer_vision_expirement/datasets/widerface/WIDER_train.zip\n",
            "100% 1.47G/1.47G [00:28<00:00, 52.3MB/s]\n",
            "  Extracting WIDER_train.zip...\n",
            "  Extracted to: /content/computer_vision_expirement/datasets/widerface\n",
            "  Downloading WIDER_val.zip from Google Drive...\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1GUCogbp16PMGa39thoMMeWxp7Rp5oM8Q\n",
            "From (redirected): https://drive.google.com/uc?id=1GUCogbp16PMGa39thoMMeWxp7Rp5oM8Q&confirm=t&uuid=633d616f-bbf9-410b-9de4-5e05e2487b67\n",
            "To: /content/computer_vision_expirement/datasets/widerface/WIDER_val.zip\n",
            "100% 363M/363M [00:06<00:00, 54.5MB/s]\n",
            "  Extracting WIDER_val.zip...\n",
            "  Extracted to: /content/computer_vision_expirement/datasets/widerface\n",
            "  Downloading wider_face_split.zip from Hugging Face mirror...\n",
            "  Downloading: https://huggingface.co/datasets/wider_face/resolve/main/data/wider_face_split.zip\n",
            "wider_face_split.zip: 3.60MB [00:02, 1.65MB/s]                \n",
            "  Saved to: /content/computer_vision_expirement/datasets/widerface/wider_face_split.zip\n",
            "  Extracting wider_face_split.zip...\n",
            "  Extracted to: /content/computer_vision_expirement/datasets/widerface\n",
            "  WiderFace download complete!\n",
            "  Location: /content/computer_vision_expirement/datasets/widerface\n",
            "\n",
            "============================================================\n",
            "DOWNLOAD SUMMARY\n",
            "============================================================\n",
            "  rtts         ->   4322 images found\n",
            "  lfw          ->  13233 images found\n",
            "  widerface    ->  16106 images found\n",
            "\n",
            "Done! Next: run preprocess_data.py\n",
            "\n",
            "--- Preprocessing ---\n",
            "Phase 1: Data Preprocessing\n",
            "Target size: (640, 640)\n",
            "Split ratio: {'train': 0.7, 'val': 0.15, 'test': 0.15}\n",
            "Chunk size: 200 images\n",
            "JPEG quality: 90\n",
            "\n",
            "============================================================\n",
            "PREPROCESSING: rtts\n",
            "============================================================\n",
            "  Found 4322 images\n",
            "  Chunk size: 200 images | JPEG quality: 90\n",
            "\n",
            "  train: 3025 images in 16 chunks\n",
            "    train: 100%|███████████████████████████| 3025/3025 [04:21<00:00, 11.55img/s]\n",
            "    -> processed: 3025 | skipped: 0 | failed: 0\n",
            "\n",
            "  val: 648 images in 4 chunks\n",
            "    val: 100%|███████████████████████████████| 648/648 [00:55<00:00, 11.66img/s]\n",
            "    -> processed: 648 | skipped: 0 | failed: 0\n",
            "\n",
            "  test: 649 images in 4 chunks\n",
            "    test: 100%|██████████████████████████████| 649/649 [00:54<00:00, 11.85img/s]\n",
            "    -> processed: 649 | skipped: 0 | failed: 0\n",
            "\n",
            "  ────────────────────────────────────────\n",
            "  rtts DONE\n",
            "    Total processed: 4322\n",
            "    Already existed: 0\n",
            "    Failed/corrupt:  0\n",
            "    train: 3025 images\n",
            "    val: 648 images\n",
            "    test: 649 images\n",
            "  Output: /content/computer_vision_expirement/datasets/rtts_processed\n",
            "\n",
            "============================================================\n",
            "PREPROCESSING: lfw\n",
            "============================================================\n",
            "  Found 13233 images\n",
            "  Chunk size: 200 images | JPEG quality: 90\n",
            "\n",
            "  train: 9263 images in 47 chunks\n",
            "    train: 100%|██████████████████████████| 9263/9263 [00:37<00:00, 245.16img/s]\n",
            "    -> processed: 9263 | skipped: 0 | failed: 0\n",
            "\n",
            "  val: 1984 images in 10 chunks\n",
            "    val: 100%|████████████████████████████| 1984/1984 [00:07<00:00, 259.60img/s]\n",
            "    -> processed: 1984 | skipped: 0 | failed: 0\n",
            "\n",
            "  test: 1986 images in 10 chunks\n",
            "    test: 100%|███████████████████████████| 1986/1986 [00:06<00:00, 302.72img/s]\n",
            "    -> processed: 1986 | skipped: 0 | failed: 0\n",
            "\n",
            "  ────────────────────────────────────────\n",
            "  lfw DONE\n",
            "    Total processed: 13233\n",
            "    Already existed: 0\n",
            "    Failed/corrupt:  0\n",
            "    train: 9263 images\n",
            "    val: 1984 images\n",
            "    test: 1986 images\n",
            "  Output: /content/computer_vision_expirement/datasets/lfw_processed\n",
            "\n",
            "============================================================\n",
            "PREPROCESSING: widerface\n",
            "============================================================\n",
            "  Found 16106 images\n",
            "  Chunk size: 200 images | JPEG quality: 90\n",
            "\n",
            "  train: 11274 images in 57 chunks\n",
            "    train: 100%|████████████████████████| 11274/11274 [01:41<00:00, 111.16img/s]\n",
            "    -> processed: 11274 | skipped: 0 | failed: 0\n",
            "\n",
            "  val: 2415 images in 13 chunks\n",
            "    val: 100%|████████████████████████████| 2415/2415 [00:19<00:00, 120.83img/s]\n",
            "    -> processed: 2415 | skipped: 0 | failed: 0\n",
            "\n",
            "  test: 2417 images in 13 chunks\n",
            "    test: 100%|███████████████████████████| 2417/2417 [00:20<00:00, 116.31img/s]\n",
            "    -> processed: 2417 | skipped: 0 | failed: 0\n",
            "\n",
            "  ────────────────────────────────────────\n",
            "  widerface DONE\n",
            "    Total processed: 16106\n",
            "    Already existed: 0\n",
            "    Failed/corrupt:  0\n",
            "    train: 11274 images\n",
            "    val: 2415 images\n",
            "    test: 2417 images\n",
            "  Output: /content/computer_vision_expirement/datasets/widerface_processed\n",
            "\n",
            "============================================================\n",
            "PREPROCESSING COMPLETE\n",
            "============================================================\n",
            "Next: run augment_data.py\n",
            "\n",
            "Datasets ready at: /content/computer_vision_expirement/datasets\n",
            "Results will be saved to Drive: /content/drive/MyDrive/computer_vision/results/phase2\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "PROJECT_DIR = '/content/drive/MyDrive/computer_vision'\n",
        "RESULTS_DIR = f'{PROJECT_DIR}/results/phase2'\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# Clone repo and download datasets to LOCAL disk (fast SSD, not Drive)\n",
        "%cd /content\n",
        "!rm -rf computer_vision_expirement\n",
        "!git clone https://github.com/Ib-Programmer/computer_vision_expirement.git\n",
        "%cd computer_vision_expirement\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "# Download and preprocess datasets locally\n",
        "print(\"\\n--- Downloading datasets to local disk ---\")\n",
        "!python scripts/download_datasets.py rtts lfw widerface\n",
        "print(\"\\n--- Preprocessing ---\")\n",
        "!python scripts/preprocess_data.py rtts lfw widerface\n",
        "\n",
        "DATASETS_DIR = '/content/computer_vision_expirement/datasets'\n",
        "print(f\"\\nDatasets ready at: {DATASETS_DIR}\")\n",
        "print(f\"Results will be saved to Drive: {RESULTS_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rzHCOqIa13N0",
        "outputId": "fac28087-16dd-474f-80be-0b87f48bee14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision<0.16 (from versions: 0.17.0, 0.17.1, 0.17.2, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.20.0, 0.20.1, 0.21.0, 0.22.0, 0.22.1, 0.23.0, 0.24.0, 0.24.1, 0.25.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision<0.16\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q pyiqa basicsr einops\n",
        "!pip install -q transformers\n",
        "!pip install -q 'torchvision<0.16'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14KbnWgs13N0"
      },
      "source": [
        "## 2.1 Load Test Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-LXborRV13N1",
        "outputId": "d9e1317e-76a9-4f4f-a864-0ffafb4508ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100 test images\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "# Load sample test images from each dataset\n",
        "def load_test_samples(dataset_dir, max_samples=50):\n",
        "    images = []\n",
        "    paths = sorted(glob.glob(f'{dataset_dir}/*_processed/test/*.jpg') +\n",
        "                   glob.glob(f'{dataset_dir}/*_processed/test/*.png'))\n",
        "    for p in paths[:max_samples]:\n",
        "        img = cv2.imread(p)\n",
        "        if img is not None:\n",
        "            images.append((p, img))\n",
        "    return images\n",
        "\n",
        "test_images = load_test_samples(DATASETS_DIR, max_samples=100)\n",
        "print(f\"Loaded {len(test_images)} test images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnD63TSB13N1"
      },
      "source": [
        "## 2.2 Setup Enhancement Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9jFjUJXL13N1",
        "outputId": "d5f48464-9f0e-4c05-b1ae-0f22bac9c8f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Zero-DCE++ on 100 images (cuda)...\n",
            "  Epoch 10/30, Loss: 0.9068\n",
            "  Epoch 20/30, Loss: 0.8497\n",
            "  Epoch 30/30, Loss: 0.8259\n",
            "Zero-DCE++ trained! Parameters: 11,926\n"
          ]
        }
      ],
      "source": [
        "# ── Zero-DCE++ (Low-Light Enhancement) ──\n",
        "# Tiny model (~79K params), zero-reference training (no paired data needed)\n",
        "# We define it inline and train on our images in ~3 minutes\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "class CSDN(nn.Module):\n",
        "    \"\"\"Depthwise separable convolution.\"\"\"\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.depth_conv = nn.Conv2d(in_ch, in_ch, 3, 1, 1, groups=in_ch)\n",
        "        self.point_conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "    def forward(self, x):\n",
        "        return self.point_conv(self.depth_conv(x))\n",
        "\n",
        "class ZeroDCEpp(nn.Module):\n",
        "    \"\"\"Zero-DCE++ network for low-light image enhancement.\"\"\"\n",
        "    def __init__(self, scale_factor=1):\n",
        "        super().__init__()\n",
        "        n = 32\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.scale_factor = scale_factor\n",
        "        self.e_conv1 = CSDN(3, n)\n",
        "        self.e_conv2 = CSDN(n, n)\n",
        "        self.e_conv3 = CSDN(n, n)\n",
        "        self.e_conv4 = CSDN(n, n)\n",
        "        self.e_conv5 = CSDN(n*2, n)\n",
        "        self.e_conv6 = CSDN(n*2, n)\n",
        "        self.e_conv7 = CSDN(n*2, 24)  # 8 iterations * 3 RGB channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.scale_factor != 1:\n",
        "            x_down = F.interpolate(x, scale_factor=1/self.scale_factor, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            x_down = x\n",
        "        x1 = self.relu(self.e_conv1(x_down))\n",
        "        x2 = self.relu(self.e_conv2(x1))\n",
        "        x3 = self.relu(self.e_conv3(x2))\n",
        "        x4 = self.relu(self.e_conv4(x3))\n",
        "        x5 = self.relu(self.e_conv5(torch.cat([x3, x4], 1)))\n",
        "        x6 = self.relu(self.e_conv6(torch.cat([x2, x5], 1)))\n",
        "        x_r = torch.tanh(self.e_conv7(torch.cat([x1, x6], 1)))\n",
        "        if self.scale_factor != 1:\n",
        "            x_r = F.interpolate(x_r, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
        "        # Apply 8 curve iterations\n",
        "        curves = torch.split(x_r, 3, dim=1)\n",
        "        enhanced = x\n",
        "        for curve in curves:\n",
        "            enhanced = enhanced + curve * (torch.pow(enhanced, 2) - enhanced)\n",
        "        return enhanced, x_r\n",
        "\n",
        "# Zero-reference losses (no paired data needed)\n",
        "def color_constancy_loss(img):\n",
        "    mean_rgb = torch.mean(img, dim=[2, 3])\n",
        "    mr, mg, mb = mean_rgb[:, 0], mean_rgb[:, 1], mean_rgb[:, 2]\n",
        "    return torch.mean((mr - mg)**2 + (mr - mb)**2 + (mg - mb)**2)\n",
        "\n",
        "def exposure_loss(img, target_E=0.6):\n",
        "    patches = F.avg_pool2d(img, 16)\n",
        "    return torch.mean((patches - target_E)**2)\n",
        "\n",
        "def tv_loss(x_r):\n",
        "    return torch.mean(torch.abs(x_r[:, :, :, :-1] - x_r[:, :, :, 1:])) + \\\n",
        "           torch.mean(torch.abs(x_r[:, :, :-1, :] - x_r[:, :, 1:, :]))\n",
        "\n",
        "# Quick training on our images (~3 min)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "zero_dce = ZeroDCEpp(scale_factor=1).to(device)\n",
        "optimizer = torch.optim.Adam(zero_dce.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToPILImage(), transforms.Resize((256, 256)),\n",
        "                                  transforms.ToTensor()])\n",
        "\n",
        "print(f\"Training Zero-DCE++ on {len(test_images)} images ({device})...\")\n",
        "zero_dce.train()\n",
        "for epoch in range(30):\n",
        "    total_loss = 0\n",
        "    for _, img in test_images[:50]:\n",
        "        img_t = transform(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)).unsqueeze(0).to(device)\n",
        "        enhanced, curves = zero_dce(img_t)\n",
        "        loss = 10 * exposure_loss(enhanced) + 5 * color_constancy_loss(enhanced) + 200 * tv_loss(curves)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"  Epoch {epoch+1}/30, Loss: {total_loss/50:.4f}\")\n",
        "\n",
        "zero_dce.eval()\n",
        "print(f\"Zero-DCE++ trained! Parameters: {sum(p.numel() for p in zero_dce.parameters()):,}\")\n",
        "\n",
        "def enhance_zero_dce(img_bgr):\n",
        "    \"\"\"Enhance a single BGR image with Zero-DCE++.\"\"\"\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    img_t = transforms.ToTensor()(img_rgb).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        enhanced, _ = zero_dce(img_t)\n",
        "    enhanced = enhanced.squeeze(0).cpu().clamp(0, 1).permute(1, 2, 0).numpy()\n",
        "    return cv2.cvtColor((enhanced * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5m7yrB-613N1",
        "outputId": "e89f1ecf-b177-4744-ee8d-71ac150ad833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restormer already cloned\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchvision.transforms.functional_tensor'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1635933970.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Restormer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbasicsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marchs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestormer_arch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRestormer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m restormer = Restormer(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/basicsr/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# flake8: noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marchs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/basicsr/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdataset_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_dataset.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# import all the dataset modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0m_dataset_modules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'basicsr.data.{file_name}'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_filenames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/basicsr/data/realesrgan_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbasicsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegradations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcircular_lowpass_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_mixed_kernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbasicsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbasicsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileClient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_root_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimfrombytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg2tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/basicsr/data/degradations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrgb_to_grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# -------------------------------------------------------------------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision.transforms.functional_tensor'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# ── Restormer (Deraining / General Restoration) ──\n",
        "# Download weights from Hugging Face, load model from cloned repo\n",
        "!git clone https://github.com/swz30/Restormer.git 2>/dev/null || echo \"Restormer already cloned\"\n",
        "\n",
        "import os\n",
        "os.makedirs('weights', exist_ok=True)\n",
        "if not os.path.exists('weights/restormer_deraining.pth'):\n",
        "    print(\"Downloading Restormer weights from Hugging Face...\")\n",
        "    !wget -q -O weights/restormer_deraining.pth \"https://huggingface.co/deepinv/Restormer/resolve/main/deraining.pth\"\n",
        "    print(f\"Downloaded: {os.path.getsize('weights/restormer_deraining.pth') / 1e6:.1f} MB\")\n",
        "\n",
        "# Load Restormer model\n",
        "import sys\n",
        "sys.path.insert(0, 'Restormer')\n",
        "from basicsr.models.archs.restormer_arch import Restormer\n",
        "\n",
        "restormer = Restormer(\n",
        "    inp_channels=3, out_channels=3, dim=48,\n",
        "    num_blocks=[4, 6, 6, 8], num_refinement_blocks=4,\n",
        "    heads=[1, 2, 4, 8], ffn_expansion_factor=2.66, bias=False,\n",
        "    LayerNorm_type='WithBias', dual_pixel_task=False\n",
        ").to(device)\n",
        "\n",
        "checkpoint = torch.load('weights/restormer_deraining.pth', map_location=device)\n",
        "restormer.load_state_dict(checkpoint['params'] if 'params' in checkpoint else checkpoint)\n",
        "restormer.eval()\n",
        "print(f\"Restormer loaded! Parameters: {sum(p.numel() for p in restormer.parameters()):,}\")\n",
        "\n",
        "def enhance_restormer(img_bgr):\n",
        "    \"\"\"Enhance a single BGR image with Restormer.\"\"\"\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "    img_t = torch.from_numpy(img_rgb).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "    # Pad to multiple of 8\n",
        "    _, _, h, w = img_t.shape\n",
        "    pad_h = (8 - h % 8) % 8\n",
        "    pad_w = (8 - w % 8) % 8\n",
        "    img_t = F.pad(img_t, (0, pad_w, 0, pad_h), mode='reflect')\n",
        "    with torch.no_grad():\n",
        "        output = restormer(img_t)\n",
        "    output = output[:, :, :h, :w].squeeze(0).cpu().clamp(0, 1).permute(1, 2, 0).numpy()\n",
        "    return cv2.cvtColor((output * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eDEae0jt13N1",
        "outputId": "e1fb3351-4f8b-4637-db65-519699a5e68b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'MaximForImageDenoising' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3225551821.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# FFA-Net weights are behind Baidu/Kaggle auth walls - MAXIM is newer and better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoImageProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaximForImageDenoising\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading MAXIM dehazing model from Hugging Face...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'MaximForImageDenoising' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# ── MAXIM (Dehazing) ──\n",
        "# Google's MAXIM model for outdoor dehazing, from Hugging Face (replaces FFA-Net)\n",
        "# FFA-Net weights are behind Baidu/Kaggle auth walls - MAXIM is newer and better\n",
        "\n",
        "from transformers import AutoImageProcessor, MaximForImageDenoising\n",
        "\n",
        "print(\"Loading MAXIM dehazing model from Hugging Face...\")\n",
        "maxim_processor = AutoImageProcessor.from_pretrained(\"google/maxim-s2-dehazing-sots-outdoor\")\n",
        "maxim_model = MaximForImageDenoising.from_pretrained(\"google/maxim-s2-dehazing-sots-outdoor\").to(device)\n",
        "maxim_model.eval()\n",
        "print(f\"MAXIM loaded! Parameters: {sum(p.numel() for p in maxim_model.parameters()):,}\")\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def enhance_maxim(img_bgr):\n",
        "    \"\"\"Dehaze a single BGR image with MAXIM.\"\"\"\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    pil_img = Image.fromarray(img_rgb)\n",
        "    inputs = maxim_processor(images=pil_img, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = maxim_model(**inputs)\n",
        "    output = outputs.reconstruction.squeeze(0).cpu().clamp(0, 1).permute(1, 2, 0).numpy()\n",
        "    output = cv2.resize((output * 255).astype(np.uint8), (img_bgr.shape[1], img_bgr.shape[0]))\n",
        "    return cv2.cvtColor(output, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Enhancement Models Ready:\")\n",
        "print(f\"  1. Zero-DCE++  (low-light)  - trained on our data\")\n",
        "print(f\"  2. Restormer   (deraining)  - pretrained from HuggingFace\")\n",
        "print(f\"  3. MAXIM       (dehazing)   - pretrained from HuggingFace\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n68apxq13N2"
      },
      "source": [
        "## 2.3 Run Enhancement & Measure Quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhykortG13N2",
        "outputId": "0b6f8671-9d44-4770-c5df-88e7b60b0f25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://huggingface.co/chaofengc/IQA-PyTorch-Weights/resolve/main/niqe_modelparameters.mat\" to /root/.cache/torch/hub/pyiqa/niqe_modelparameters.mat\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8.15k/8.15k [00:00<00:00, 25.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "import pyiqa\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "# Initialize no-reference metrics\n",
        "niqe_metric = pyiqa.create_metric('niqe', device='cuda' if __import__('torch').cuda.is_available() else 'cpu')\n",
        "\n",
        "def evaluate_image_quality(original, enhanced):\n",
        "    \"\"\"Calculate quality metrics between original and enhanced images.\"\"\"\n",
        "    # Convert to float\n",
        "    orig_f = original.astype(np.float64) / 255.0\n",
        "    enh_f = enhanced.astype(np.float64) / 255.0\n",
        "\n",
        "    metrics = {}\n",
        "    metrics['psnr'] = psnr(orig_f, enh_f, data_range=1.0)\n",
        "    metrics['ssim'] = ssim(orig_f, enh_f, data_range=1.0, channel_axis=2)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def measure_inference_time(model_fn, image, n_runs=10):\n",
        "    \"\"\"Measure average inference time.\"\"\"\n",
        "    times = []\n",
        "    for _ in range(n_runs):\n",
        "        start = time.time()\n",
        "        _ = model_fn(image)\n",
        "        times.append(time.time() - start)\n",
        "    return np.mean(times) * 1000  # ms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a3ZdsZV13N2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Run all 3 models on test images and compute metrics\n",
        "models = {\n",
        "    'Zero-DCE++': enhance_zero_dce,\n",
        "    'Restormer': enhance_restormer,\n",
        "    'MAXIM': enhance_maxim,\n",
        "}\n",
        "\n",
        "all_results = {name: {'psnr': [], 'ssim': [], 'niqe': [], 'latency': []} for name in models}\n",
        "\n",
        "n_eval = min(30, len(test_images))  # evaluate on 30 images\n",
        "print(f\"Evaluating {len(models)} models on {n_eval} images...\")\n",
        "\n",
        "for idx, (path, img) in enumerate(test_images[:n_eval]):\n",
        "    if (idx + 1) % 10 == 0:\n",
        "        print(f\"  Processing image {idx+1}/{n_eval}...\")\n",
        "\n",
        "    for name, enhance_fn in models.items():\n",
        "        try:\n",
        "            # Measure latency\n",
        "            start = time.time()\n",
        "            enhanced = enhance_fn(img)\n",
        "            latency = (time.time() - start) * 1000\n",
        "\n",
        "            # Resize enhanced to match original if needed\n",
        "            if enhanced.shape[:2] != img.shape[:2]:\n",
        "                enhanced = cv2.resize(enhanced, (img.shape[1], img.shape[0]))\n",
        "\n",
        "            # Compute metrics\n",
        "            metrics = evaluate_image_quality(img, enhanced)\n",
        "            all_results[name]['psnr'].append(metrics['psnr'])\n",
        "            all_results[name]['ssim'].append(metrics['ssim'])\n",
        "            all_results[name]['latency'].append(latency)\n",
        "\n",
        "            # NIQE (no-reference quality)\n",
        "            enh_tensor = torch.from_numpy(cv2.cvtColor(enhanced, cv2.COLOR_BGR2RGB)).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
        "            niqe_score = niqe_metric(enh_tensor.to(device)).item()\n",
        "            all_results[name]['niqe'].append(niqe_score)\n",
        "        except Exception as e:\n",
        "            print(f\"  [WARN] {name} failed on image {idx}: {e}\")\n",
        "\n",
        "# Build comparison table\n",
        "rows = []\n",
        "for name in models:\n",
        "    r = all_results[name]\n",
        "    if r['psnr']:\n",
        "        rows.append({\n",
        "            'Model': name,\n",
        "            'Avg_PSNR': round(np.mean(r['psnr']), 2),\n",
        "            'Avg_SSIM': round(np.mean(r['ssim']), 4),\n",
        "            'Avg_NIQE': round(np.mean(r['niqe']), 2),\n",
        "            'Avg_Latency_ms': round(np.mean(r['latency']), 1),\n",
        "        })\n",
        "\n",
        "evaluation_df = pd.DataFrame(rows)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ENHANCEMENT MODEL COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(evaluation_df.to_string(index=False))\n",
        "evaluation_df.to_csv(f'{RESULTS_DIR}/enhancement_benchmark.csv', index=False)\n",
        "print(f\"\\nResults saved to: {RESULTS_DIR}/enhancement_benchmark.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCCGaDe213N2"
      },
      "source": [
        "## 2.4 Results Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eytgc1D-13N3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visual comparison on 3 sample images\n",
        "samples = test_images[:3]\n",
        "model_names = list(models.keys())\n",
        "\n",
        "fig, axes = plt.subplots(len(samples), len(model_names) + 1, figsize=(5 * (len(model_names) + 1), 5 * len(samples)))\n",
        "\n",
        "for row, (path, img) in enumerate(samples):\n",
        "    # Original\n",
        "    axes[row][0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    axes[row][0].set_title('Original', fontsize=12)\n",
        "    axes[row][0].axis('off')\n",
        "\n",
        "    # Enhanced by each model\n",
        "    for col, name in enumerate(model_names, 1):\n",
        "        try:\n",
        "            enhanced = models[name](img)\n",
        "            if enhanced.shape[:2] != img.shape[:2]:\n",
        "                enhanced = cv2.resize(enhanced, (img.shape[1], img.shape[0]))\n",
        "            axes[row][col].imshow(cv2.cvtColor(enhanced, cv2.COLOR_BGR2RGB))\n",
        "        except:\n",
        "            axes[row][col].text(0.5, 0.5, 'Failed', ha='center', va='center', fontsize=14)\n",
        "        axes[row][col].set_title(name, fontsize=12)\n",
        "        axes[row][col].axis('off')\n",
        "\n",
        "plt.suptitle('Image Enhancement Comparison', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{RESULTS_DIR}/enhancement_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Comparison saved to: {RESULTS_DIR}/enhancement_comparison.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTvGRR1R13N3"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nPhase 2 Complete!\")\n",
        "print(f\"Results saved to: {RESULTS_DIR}\")\n",
        "print(f\"\\nModels evaluated:\")\n",
        "print(f\"  - Zero-DCE++  : Low-light enhancement (zero-reference)\")\n",
        "print(f\"  - Restormer   : Deraining (supervised, pretrained)\")\n",
        "print(f\"  - MAXIM       : Dehazing (supervised, pretrained)\")\n",
        "print(f\"\\nNote: FFA-Net replaced with MAXIM (Google) - weights more accessible,\")\n",
        "print(f\"      newer architecture, better dehazing performance.\")\n",
        "print(f\"\\nNext: Open Phase3_Object_Detection.ipynb\")"
      ]
    }
  ]
}