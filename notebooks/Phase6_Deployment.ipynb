{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 6: End-to-End System & Deployment\n",
    "Building a REST API with FastAPI, frontend with Gradio, and Docker containerization.\n",
    "\n",
    "**Components**: FastAPI backend, Gradio UI, Docker deployment\n",
    "**Tests**: End-to-end latency, concurrent users, system stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import drive\ndrive.mount('/content/drive')\n\nimport os\nPROJECT_DIR = '/content/drive/MyDrive/computer_vision'\nRESULTS_DIR = f'{PROJECT_DIR}/results/phase6'\nMODELS_DIR = f'{PROJECT_DIR}/results'\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\n%cd /content\n!rm -rf computer_vision_expirement\n!git clone https://github.com/Ib-Programmer/computer_vision_expirement.git\n%cd computer_vision_expirement\n\n!pip install -q fastapi uvicorn python-multipart gradio\n!pip install -q ultralytics insightface onnxruntime-gpu\n\n# Download a small set of test images locally for system testing\nprint(\"\\n--- Downloading test images to local disk ---\")\n!python scripts/download_datasets.py lfw rtts\n!python scripts/preprocess_data.py lfw rtts\n\nDATASETS_DIR = '/content/computer_vision_expirement/datasets'\nprint(f\"\\nModels from previous phases: {MODELS_DIR}\")\nprint(f\"Test images: {DATASETS_DIR}\")\nprint(f\"Results will be saved to: {RESULTS_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Build the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import cv2\nimport numpy as np\nimport time\n\nclass DetectionPipeline:\n    \"\"\"End-to-end outdoor detection + face recognition pipeline.\"\"\"\n    \n    def __init__(self, det_model_path=None, face_app=None):\n        self.detector = None\n        self.face_app = None\n        \n        # Load object detector\n        if det_model_path and os.path.exists(det_model_path):\n            from ultralytics import YOLO\n            self.detector = YOLO(det_model_path)\n            print(f\"Detector loaded: {det_model_path}\")\n        \n        # Load face analyzer\n        try:\n            from insightface.app import FaceAnalysis\n            self.face_app = FaceAnalysis(name='buffalo_l', \n                                          providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n            self.face_app.prepare(ctx_id=0, det_size=(640, 640))\n            print(\"Face analyzer loaded: RetinaFace + ArcFace\")\n        except Exception as e:\n            print(f\"Face analyzer not available: {e}\")\n    \n    def process(self, image):\n        \"\"\"Run full pipeline on an image.\"\"\"\n        results = {'detections': [], 'faces': [], 'timing': {}}\n        \n        start = time.time()\n        \n        # Object detection\n        if self.detector:\n            t0 = time.time()\n            det_results = self.detector(image, verbose=False)\n            results['timing']['detection_ms'] = (time.time() - t0) * 1000\n            \n            for r in det_results:\n                for box in r.boxes:\n                    results['detections'].append({\n                        'class': r.names[int(box.cls)],\n                        'confidence': float(box.conf),\n                        'bbox': box.xyxy[0].tolist()\n                    })\n        \n        # Face recognition\n        if self.face_app:\n            t0 = time.time()\n            faces = self.face_app.get(image)\n            results['timing']['face_ms'] = (time.time() - t0) * 1000\n            \n            for face in faces:\n                results['faces'].append({\n                    'bbox': face.bbox.tolist(),\n                    'score': float(face.det_score),\n                    'embedding_dim': len(face.embedding)\n                })\n        \n        results['timing']['total_ms'] = (time.time() - start) * 1000\n        return results\n\n# Find best trained model (prefer outdoor-augmented from Phase 3)\ndet_path = None\nfor candidate in [\n    f'{MODELS_DIR}/phase3/yolov8n_outdoor_aug/weights/best.pt',\n    f'{MODELS_DIR}/phase3/yolov8n_baseline/weights/best.pt',\n    f'{MODELS_DIR}/phase3/yolov8n_raw/weights/best.pt',\n]:\n    if os.path.exists(candidate):\n        det_path = candidate\n        break\n\nif det_path is None:\n    print(\"No trained detector found - using pretrained YOLOv8n\")\n    det_path = 'yolov8n.pt'\n\npipeline = DetectionPipeline(det_model_path=det_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 FastAPI Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "from fastapi.responses import JSONResponse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import io\n",
    "import time\n",
    "\n",
    "app = FastAPI(title=\"Outdoor Detection & Face Recognition API\")\n",
    "\n",
    "# Pipeline will be initialized on startup\n",
    "pipeline = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup():\n",
    "    global pipeline\n",
    "    # Import and initialize pipeline\n",
    "    print(\"Loading models...\")\n",
    "\n",
    "@app.post(\"/detect\")\n",
    "async def detect_objects(file: UploadFile = File(...)):\n",
    "    contents = await file.read()\n",
    "    nparr = np.frombuffer(contents, np.uint8)\n",
    "    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    if image is None:\n",
    "        return JSONResponse(status_code=400, content={\"error\": \"Invalid image\"})\n",
    "    \n",
    "    # Run pipeline\n",
    "    results = pipeline.process(image) if pipeline else {\"error\": \"Pipeline not loaded\"}\n",
    "    return results\n",
    "\n",
    "@app.post(\"/recognize\")\n",
    "async def recognize_faces(file: UploadFile = File(...)):\n",
    "    contents = await file.read()\n",
    "    nparr = np.frombuffer(contents, np.uint8)\n",
    "    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    if image is None:\n",
    "        return JSONResponse(status_code=400, content={\"error\": \"Invalid image\"})\n",
    "    \n",
    "    results = pipeline.process(image) if pipeline else {\"error\": \"Pipeline not loaded\"}\n",
    "    return {\"faces\": results.get(\"faces\", []), \"timing\": results.get(\"timing\", {})}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"ok\", \"models_loaded\": pipeline is not None}\n",
    "\n",
    "print(\"FastAPI app written to app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Gradio Frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def process_image(image):\n",
    "    \"\"\"Process uploaded image through the pipeline.\"\"\"\n",
    "    if image is None:\n",
    "        return None, \"No image uploaded\"\n",
    "    \n",
    "    # Convert from RGB (Gradio) to BGR (OpenCV)\n",
    "    img_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    results = pipeline.process(img_bgr)\n",
    "    \n",
    "    # Draw detections\n",
    "    annotated = img_bgr.copy()\n",
    "    \n",
    "    for det in results['detections']:\n",
    "        bbox = [int(x) for x in det['bbox']]\n",
    "        cv2.rectangle(annotated, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "        label = f\"{det['class']} {det['confidence']:.2f}\"\n",
    "        cv2.putText(annotated, label, (bbox[0], bbox[1]-10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    for face in results['faces']:\n",
    "        bbox = [int(x) for x in face['bbox']]\n",
    "        cv2.rectangle(annotated, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 0, 0), 2)\n",
    "        cv2.putText(annotated, f\"Face {face['score']:.2f}\", (bbox[0], bbox[1]-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    \n",
    "    annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    summary = (\n",
    "        f\"Objects detected: {len(results['detections'])}\\n\"\n",
    "        f\"Faces detected: {len(results['faces'])}\\n\"\n",
    "        f\"Total time: {results['timing'].get('total_ms', 0):.1f} ms\"\n",
    "    )\n",
    "    \n",
    "    return annotated_rgb, summary\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=process_image,\n",
    "    inputs=gr.Image(label=\"Upload Image\"),\n",
    "    outputs=[\n",
    "        gr.Image(label=\"Detection Results\"),\n",
    "        gr.Textbox(label=\"Summary\")\n",
    "    ],\n",
    "    title=\"Outdoor Object Detection & Face Recognition\",\n",
    "    description=\"Upload an outdoor image to detect objects and recognize faces.\"\n",
    ")\n",
    "\n",
    "# Launch (creates a public URL in Colab)\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Docker Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y libgl1-mesa-glx libglib2.0-0 && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "RUN pip install --no-cache-dir fastapi uvicorn python-multipart\n",
    "\n",
    "COPY . .\n",
    "\n",
    "EXPOSE 8000\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-compose.yml\n",
    "version: '3.8'\n",
    "services:\n",
    "  api:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    volumes:\n",
    "      - ./weights:/app/weights\n",
    "    environment:\n",
    "      - CUDA_VISIBLE_DEVICES=0\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - capabilities: [gpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 System Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import glob\n\nprint(\"=\" * 60)\nprint(\"END-TO-END SYSTEM TEST\")\nprint(\"=\" * 60)\n\n# Test with sample images from LOCAL disk\ntest_images = glob.glob(f'{DATASETS_DIR}/*_processed/test/*.jpg')[:20]\n\nlatencies = []\nfor img_path in test_images:\n    img = cv2.imread(img_path)\n    if img is None:\n        continue\n    \n    start = time.time()\n    results = pipeline.process(img)\n    elapsed = (time.time() - start) * 1000\n    latencies.append(elapsed)\n\nif latencies:\n    import pandas as pd\n    \n    system_metrics = {\n        'Metric': ['Avg Latency', 'P50 Latency', 'P95 Latency', 'Min Latency', 'Max Latency', 'Throughput'],\n        'Value': [\n            f'{np.mean(latencies):.1f} ms',\n            f'{np.percentile(latencies, 50):.1f} ms',\n            f'{np.percentile(latencies, 95):.1f} ms',\n            f'{np.min(latencies):.1f} ms',\n            f'{np.max(latencies):.1f} ms',\n            f'{1000/np.mean(latencies):.1f} FPS'\n        ]\n    }\n    \n    df = pd.DataFrame(system_metrics)\n    print(df.to_string(index=False))\n    df.to_csv(f'{RESULTS_DIR}/system_benchmark.csv', index=False)\n    \n    print(f\"\\nImages tested: {len(latencies)}\")\n    print(f\"Target: < 2000ms per image\")\n    print(f\"Result: {'PASS' if np.mean(latencies) < 2000 else 'NEEDS OPTIMIZATION'}\")\nelse:\n    print(\"No test images found. Check dataset download.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PROJECT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"All results saved to: {PROJECT_DIR}/results/\")\n",
    "print(\"\"\"\n",
    "Phase Summary:\n",
    "  Phase 1: Data prepared and augmented\n",
    "  Phase 2: Enhancement models evaluated\n",
    "  Phase 3: Detection models trained and compared\n",
    "  Phase 4: Face recognition pipeline validated\n",
    "  Phase 5: Models optimized (ONNX/TensorRT)\n",
    "  Phase 6: API + UI deployed and tested\n",
    "\n",
    "Artifacts:\n",
    "  - Trained models: results/phase3/\n",
    "  - Optimized models: results/phase5/\n",
    "  - Benchmark reports: results/phase*/\n",
    "  - Docker config: Dockerfile, docker-compose.yml\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}