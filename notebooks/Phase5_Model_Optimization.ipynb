{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Phase 5: Model Optimization & Acceleration\n",
    "Export models to ONNX, optimize with TensorRT, apply INT8 quantization.\n",
    "\n",
    "**Goal**: Maximize inference speed while minimizing accuracy loss\n",
    "**Benchmark**: FP32 vs FP16 vs INT8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "PROJECT_DIR = '/content/drive/MyDrive/computer_vision'\n",
    "RESULTS_DIR = f'{PROJECT_DIR}/results/phase5'\n",
    "MODELS_DIR = f'{PROJECT_DIR}/results/phase3'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "!pip install -q ultralytics onnx onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## 5.1 Export YOLOv8 to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [],
   "source": "from ultralytics import YOLO\nimport shutil\n\n# Find best model from Phase 3 (prefer outdoor-augmented)\nbest_pt = f'{MODELS_DIR}/yolov8n_outdoor_aug/weights/best.pt'\nif not os.path.exists(best_pt):\n    best_pt = f'{MODELS_DIR}/yolov8n_baseline/weights/best.pt'\nif not os.path.exists(best_pt):\n    best_pt = f'{MODELS_DIR}/yolov8n_raw/weights/best.pt'\n\nif os.path.exists(best_pt):\n    print(f\"Using model: {best_pt}\")\n    model = YOLO(best_pt)\n    \n    # ONNX export\n    onnx_path = model.export(format='onnx', imgsz=640, simplify=True)\n    print(f\"ONNX model exported: {onnx_path}\")\n    \n    # Copy to results\n    shutil.copy(onnx_path, f'{RESULTS_DIR}/yolov8n_best.onnx')\nelse:\n    print(f\"No trained model found.\")\n    print(\"Run Phase 3 first to train the model.\")"
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8",
   "metadata": {},
   "source": [
    "## 5.2 Export to TensorRT (FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(best_pt):\n",
    "    model = YOLO(best_pt)\n",
    "    \n",
    "    # TensorRT FP16 export\n",
    "    trt_path = model.export(format='engine', imgsz=640, half=True)\n",
    "    print(f\"TensorRT FP16 model exported: {trt_path}\")\n",
    "    \n",
    "    shutil.copy(trt_path, f'{RESULTS_DIR}/yolov8n_fp16.engine')\n",
    "else:\n",
    "    print(\"Skipping TensorRT export - model not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "## 5.3 ONNX Runtime Inference Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "onnx_model_path = f'{RESULTS_DIR}/yolov8n_best.onnx'\n",
    "\n",
    "if os.path.exists(onnx_model_path):\n",
    "    # Create sessions with different providers\n",
    "    providers_list = {\n",
    "        'ONNX_CPU': ['CPUExecutionProvider'],\n",
    "        'ONNX_GPU': ['CUDAExecutionProvider', 'CPUExecutionProvider'],\n",
    "    }\n",
    "    \n",
    "    # Create dummy input\n",
    "    dummy = np.random.randn(1, 3, 640, 640).astype(np.float32)\n",
    "    \n",
    "    for name, providers in providers_list.items():\n",
    "        try:\n",
    "            session = ort.InferenceSession(onnx_model_path, providers=providers)\n",
    "            input_name = session.get_inputs()[0].name\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(5):\n",
    "                session.run(None, {input_name: dummy})\n",
    "            \n",
    "            # Benchmark\n",
    "            times = []\n",
    "            for _ in range(50):\n",
    "                start = time.time()\n",
    "                session.run(None, {input_name: dummy})\n",
    "                times.append((time.time() - start) * 1000)\n",
    "            \n",
    "            avg = np.mean(times)\n",
    "            fps = 1000 / avg\n",
    "            print(f\"{name}: {avg:.1f} ms/img ({fps:.1f} FPS)\")\n",
    "        except Exception as e:\n",
    "            print(f\"{name}: failed - {e}\")\n",
    "else:\n",
    "    print(\"ONNX model not found. Export it first (section 5.1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## 5.4 PyTorch vs ONNX vs TensorRT Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "benchmark_results = []\n",
    "\n",
    "# PyTorch benchmark\n",
    "if os.path.exists(best_pt):\n",
    "    model = YOLO(best_pt)\n",
    "    dummy_img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        model(dummy_img, verbose=False)\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(50):\n",
    "        start = time.time()\n",
    "        model(dummy_img, verbose=False)\n",
    "        times.append((time.time() - start) * 1000)\n",
    "    \n",
    "    avg_pt = np.mean(times)\n",
    "    benchmark_results.append({\n",
    "        'Format': 'PyTorch FP32',\n",
    "        'Latency_ms': round(avg_pt, 1),\n",
    "        'FPS': round(1000/avg_pt, 1),\n",
    "        'Model_Size_MB': round(os.path.getsize(best_pt) / 1e6, 1)\n",
    "    })\n",
    "\n",
    "# ONNX benchmark\n",
    "onnx_path = f'{RESULTS_DIR}/yolov8n_best.onnx'\n",
    "if os.path.exists(onnx_path):\n",
    "    session = ort.InferenceSession(onnx_path, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    dummy = np.random.randn(1, 3, 640, 640).astype(np.float32)\n",
    "    \n",
    "    for _ in range(5):\n",
    "        session.run(None, {input_name: dummy})\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(50):\n",
    "        start = time.time()\n",
    "        session.run(None, {input_name: dummy})\n",
    "        times.append((time.time() - start) * 1000)\n",
    "    \n",
    "    avg_onnx = np.mean(times)\n",
    "    benchmark_results.append({\n",
    "        'Format': 'ONNX Runtime GPU',\n",
    "        'Latency_ms': round(avg_onnx, 1),\n",
    "        'FPS': round(1000/avg_onnx, 1),\n",
    "        'Model_Size_MB': round(os.path.getsize(onnx_path) / 1e6, 1)\n",
    "    })\n",
    "\n",
    "# TensorRT benchmark\n",
    "trt_path = f'{RESULTS_DIR}/yolov8n_fp16.engine'\n",
    "if os.path.exists(trt_path):\n",
    "    model_trt = YOLO(trt_path)\n",
    "    \n",
    "    for _ in range(5):\n",
    "        model_trt(dummy_img, verbose=False)\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(50):\n",
    "        start = time.time()\n",
    "        model_trt(dummy_img, verbose=False)\n",
    "        times.append((time.time() - start) * 1000)\n",
    "    \n",
    "    avg_trt = np.mean(times)\n",
    "    benchmark_results.append({\n",
    "        'Format': 'TensorRT FP16',\n",
    "        'Latency_ms': round(avg_trt, 1),\n",
    "        'FPS': round(1000/avg_trt, 1),\n",
    "        'Model_Size_MB': round(os.path.getsize(trt_path) / 1e6, 1)\n",
    "    })\n",
    "\n",
    "if benchmark_results:\n",
    "    df = pd.DataFrame(benchmark_results)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"OPTIMIZATION BENCHMARK\")\n",
    "    print(\"=\" * 60)\n",
    "    print(df.to_string(index=False))\n",
    "    df.to_csv(f'{RESULTS_DIR}/optimization_benchmark.csv', index=False)\n",
    "else:\n",
    "    print(\"No models found. Run Phases 3 & 5.1 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nPhase 5 results saved to: {RESULTS_DIR}\")\n",
    "print(\"Optimized models ready for deployment.\")\n",
    "print(\"Next: Open Phase6_Deployment.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}