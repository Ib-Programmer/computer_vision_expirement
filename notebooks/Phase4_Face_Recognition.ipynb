{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Face Detection & Recognition Evaluation\n",
    "Using RetinaFace for detection, ArcFace for feature extraction, and FAISS for matching.\n",
    "\n",
    "**Metrics**: Recognition Accuracy, FAR, FRR, Latency\n",
    "**Datasets**: LFW, WiderFace (preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from google.colab import drive\ndrive.mount('/content/drive')\n\nimport os\nPROJECT_DIR = '/content/drive/MyDrive/computer_vision'\nRESULTS_DIR = f'{PROJECT_DIR}/results/phase4'\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\n# Clone repo and download datasets to LOCAL disk (fast SSD)\n%cd /content\n!rm -rf computer_vision_expirement\n!git clone https://github.com/Ib-Programmer/computer_vision_expirement.git\n%cd computer_vision_expirement\n\n!pip install -q insightface onnxruntime-gpu faiss-gpu albumentations\n!pip install -q -r requirements.txt\n\n# Download and preprocess datasets locally\nprint(\"\\n--- Downloading datasets to local disk ---\")\n!python scripts/download_datasets.py lfw widerface\nprint(\"\\n--- Preprocessing ---\")\n!python scripts/preprocess_data.py lfw widerface\n\nDATASETS_DIR = '/content/computer_vision_expirement/datasets'\nprint(f\"\\nDatasets ready at: {DATASETS_DIR}\")\nprint(f\"Results will be saved to Drive: {RESULTS_DIR}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Face Detection with RetinaFace"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# Initialize face analysis (RetinaFace + ArcFace)\n",
    "app = FaceAnalysis(name='buffalo_l', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "print(\"InsightFace loaded: RetinaFace (detection) + ArcFace (recognition)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import glob\n",
    "import time\n",
    "\n",
    "lfw_test = sorted(glob.glob(f'{DATASETS_DIR}/lfw_processed/test/*.jpg'))\n",
    "print(f\"LFW test images: {len(lfw_test)}\")\n",
    "\n",
    "detection_results = []\n",
    "detection_times = []\n",
    "\n",
    "for img_path in lfw_test[:200]:  # Process 200 images\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        continue\n",
    "    \n",
    "    start = time.time()\n",
    "    faces = app.get(img)\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "    \n",
    "    detection_times.append(elapsed)\n",
    "    detection_results.append({\n",
    "        'path': img_path,\n",
    "        'num_faces': len(faces),\n",
    "        'time_ms': elapsed\n",
    "    })\n",
    "\n",
    "print(f\"Processed: {len(detection_results)} images\")\n",
    "print(f\"Avg detection time: {np.mean(detection_times):.1f} ms\")\n",
    "print(f\"Faces found: {sum(r['num_faces'] for r in detection_results)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feature Extraction (ArcFace Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extract face embeddings from detected faces\n",
    "embeddings_db = []\n",
    "labels_db = []\n",
    "\n",
    "print(\"Extracting face embeddings from LFW...\")\n",
    "for img_path in lfw_test[:200]:\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        continue\n",
    "    \n",
    "    faces = app.get(img)\n",
    "    for face in faces:\n",
    "        embedding = face.embedding  # 512-d ArcFace embedding\n",
    "        label = os.path.basename(os.path.dirname(img_path)) if '/' in img_path else os.path.basename(img_path)\n",
    "        \n",
    "        embeddings_db.append(embedding)\n",
    "        labels_db.append(label)\n",
    "\n",
    "embeddings_db = np.array(embeddings_db).astype('float32')\n",
    "print(f\"Extracted {len(embeddings_db)} embeddings, shape: {embeddings_db.shape}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Face Matching with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import faiss\n",
    "\n",
    "if len(embeddings_db) > 0:\n",
    "    # Normalize embeddings for cosine similarity\n",
    "    faiss.normalize_L2(embeddings_db)\n",
    "    \n",
    "    # Build FAISS index\n",
    "    dimension = embeddings_db.shape[1]  # 512\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner product (cosine after normalization)\n",
    "    index.add(embeddings_db)\n",
    "    \n",
    "    print(f\"FAISS index built: {index.ntotal} vectors, {dimension}-d\")\n",
    "    \n",
    "    # Search: query each embedding against the database\n",
    "    k = 5  # top-5 matches\n",
    "    search_start = time.time()\n",
    "    distances, indices = index.search(embeddings_db[:50], k)  # Query first 50\n",
    "    search_time = (time.time() - search_start) * 1000\n",
    "    \n",
    "    print(f\"Search time for 50 queries: {search_time:.1f} ms\")\n",
    "    print(f\"Avg per query: {search_time/50:.2f} ms\")\n",
    "    \n",
    "    # Show sample results\n",
    "    print(\"\\nSample matches (query -> top match, similarity):\")\n",
    "    for i in range(min(5, len(distances))):\n",
    "        print(f\"  Query {i}: match_idx={indices[i][1]}, similarity={distances[i][1]:.4f}\")\n",
    "else:\n",
    "    print(\"No embeddings extracted. Check face detection results.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Evaluate Recognition Performance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_recognition(embeddings, labels, thresholds=np.arange(0.1, 1.0, 0.05)):\n",
    "    \"\"\"Evaluate face recognition at different thresholds.\"\"\"\n",
    "    faiss.normalize_L2(embeddings.copy())\n",
    "    \n",
    "    results = []\n",
    "    n = len(embeddings)\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "        \n",
    "        for i in range(min(n, 100)):  # Sample pairs\n",
    "            for j in range(i+1, min(n, 100)):\n",
    "                sim = np.dot(embeddings[i], embeddings[j])\n",
    "                same_person = (labels[i] == labels[j])\n",
    "                \n",
    "                if sim >= threshold:\n",
    "                    if same_person:\n",
    "                        tp += 1\n",
    "                    else:\n",
    "                        fp += 1\n",
    "                else:\n",
    "                    if same_person:\n",
    "                        fn += 1\n",
    "                    else:\n",
    "                        tn += 1\n",
    "        \n",
    "        total_genuine = tp + fn\n",
    "        total_impostor = fp + tn\n",
    "        \n",
    "        far = fp / max(total_impostor, 1)\n",
    "        frr = fn / max(total_genuine, 1)\n",
    "        accuracy = (tp + tn) / max(tp + fp + tn + fn, 1)\n",
    "        \n",
    "        results.append({\n",
    "            'Threshold': round(threshold, 2),\n",
    "            'Accuracy': round(accuracy, 4),\n",
    "            'FAR': round(far, 4),\n",
    "            'FRR': round(frr, 4),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "if len(embeddings_db) > 0:\n",
    "    eval_df = evaluate_recognition(embeddings_db, labels_db)\n",
    "    print(\"Recognition Performance at Different Thresholds:\")\n",
    "    print(eval_df.to_string(index=False))\n",
    "    eval_df.to_csv(f'{RESULTS_DIR}/recognition_metrics.csv', index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Test Robustness Under Outdoor Conditions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import matplotlib.pyplot as plt\nimport albumentations as A\n\n# Apply outdoor augmentations ON-THE-FLY for robustness testing\nconditions = {\n    'fog': A.Compose([A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=0.7, alpha_coef=0.08, p=1.0)]),\n    'low_light': A.Compose([A.RandomBrightnessContrast(brightness_limit=(-0.5, -0.2), contrast_limit=(-0.3, 0.0), p=1.0)]),\n    'motion_blur': A.Compose([A.MotionBlur(blur_limit=(7, 15), p=1.0)]),\n    'rain': A.Compose([A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1,\n                                     drop_color=(200, 200, 200), blur_value=3, brightness_coefficient=0.7, p=1.0)]),\n}\n\n# Use LFW test images from local disk\ntest_imgs = sorted(glob.glob(f'{DATASETS_DIR}/lfw_processed/test/*.jpg'))[:50]\ncondition_results = []\n\nfor condition, transform in conditions.items():\n    detected = 0\n    total = 0\n\n    for img_path in test_imgs:\n        img = cv2.imread(img_path)\n        if img is None:\n            continue\n\n        # Apply augmentation on-the-fly\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        aug_img = transform(image=img_rgb)['image']\n        aug_bgr = cv2.cvtColor(aug_img, cv2.COLOR_RGB2BGR)\n\n        total += 1\n        faces = app.get(aug_bgr)\n        if len(faces) > 0:\n            detected += 1\n\n    rate = detected / max(total, 1) * 100\n    condition_results.append({'Condition': condition, 'Images': total, 'Detected': detected, 'Rate': f'{rate:.1f}%'})\n    print(f\"  {condition}: {detected}/{total} faces detected ({rate:.1f}%)\")\n\nif condition_results:\n    cond_df = pd.DataFrame(condition_results)\n    print(\"\\nRobustness Summary:\")\n    print(cond_df.to_string(index=False))\n    cond_df.to_csv(f'{RESULTS_DIR}/robustness_analysis.csv', index=False)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xmsvtkscmj",
   "source": "## 4.6 Literature Comparison (Published Benchmarks)\nComparing our face detection and recognition results with published benchmarks.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kcjbwlcq7fg",
   "source": "import pandas as pd\n\n# Face Recognition benchmarks on LFW\nrecognition_lit = pd.DataFrame([\n    {'Method': 'DeepFace (Taigman et al., 2014)', 'LFW_Acc%': 97.35, 'Embedding': 4096, 'Loss': 'Softmax', 'Source': 'CVPR 2014'},\n    {'Method': 'FaceNet (Schroff et al., 2015)', 'LFW_Acc%': 99.63, 'Embedding': 128, 'Loss': 'Triplet', 'Source': 'CVPR 2015'},\n    {'Method': 'SphereFace (Liu et al., 2017)', 'LFW_Acc%': 99.42, 'Embedding': 512, 'Loss': 'A-Softmax', 'Source': 'CVPR 2017'},\n    {'Method': 'CosFace (Wang et al., 2018)', 'LFW_Acc%': 99.73, 'Embedding': 512, 'Loss': 'LMCL', 'Source': 'CVPR 2018'},\n    {'Method': 'ArcFace (Deng et al., 2019)', 'LFW_Acc%': 99.83, 'Embedding': 512, 'Loss': 'Additive Angular Margin', 'Source': 'CVPR 2019'},\n    {'Method': 'AdaFace (Kim et al., 2022)', 'LFW_Acc%': 99.82, 'Embedding': 512, 'Loss': 'Adaptive Margin', 'Source': 'CVPR 2022'},\n    {'Method': 'Our ArcFace (InsightFace)', 'LFW_Acc%': None, 'Embedding': 512, 'Loss': 'ArcFace', 'Source': 'This work'},\n])\n\n# Face Detection benchmarks on WiderFace\ndetection_lit = pd.DataFrame([\n    {'Method': 'MTCNN (Zhang et al., 2016)', 'Easy': 84.8, 'Medium': 82.5, 'Hard': 60.7, 'Source': 'IEEE SPL 2016'},\n    {'Method': 'S3FD (Zhang et al., 2017)', 'Easy': 93.7, 'Medium': 92.4, 'Hard': 85.2, 'Source': 'ICCV 2017'},\n    {'Method': 'DSFD (Li et al., 2019)', 'Easy': 96.6, 'Medium': 95.7, 'Hard': 90.4, 'Source': 'CVPR 2019'},\n    {'Method': 'RetinaFace (Deng et al., 2020)', 'Easy': 96.9, 'Medium': 96.1, 'Hard': 91.4, 'Source': 'CVPR 2020'},\n    {'Method': 'TinaFace (Zhu et al., 2020)', 'Easy': 96.3, 'Medium': 95.7, 'Hard': 92.2, 'Source': 'arXiv 2020'},\n    {'Method': 'SCRFD (Guo et al., 2022)', 'Easy': 96.1, 'Medium': 94.9, 'Hard': 88.5, 'Source': 'arXiv 2022'},\n    {'Method': 'Our RetinaFace (InsightFace)', 'Easy': None, 'Medium': None, 'Hard': None, 'Source': 'This work'},\n])\n\nprint('=' * 80)\nprint('TABLE 4.1: Face Recognition — Published Benchmarks (LFW Accuracy %)')\nprint('=' * 80)\nprint(recognition_lit.to_string(index=False))\n\nprint('\\n' + '=' * 80)\nprint('TABLE 4.2: Face Detection — Published Benchmarks (WiderFace AP %)')\nprint('=' * 80)\nprint(detection_lit.to_string(index=False))\n\n# FAISS retrieval benchmarks\nfaiss_lit = pd.DataFrame([\n    {'Index_Type': 'Flat (brute-force)', 'Recall@1': '100%', 'Search_Time': '~1ms/query (1K vectors)', 'Notes': 'Exact, no approximation'},\n    {'Index_Type': 'IVF1024', 'Recall@1': '~95%', 'Search_Time': '~0.1ms/query (1M vectors)', 'Notes': 'Approximate, requires training'},\n    {'Index_Type': 'HNSW', 'Recall@1': '~98%', 'Search_Time': '~0.05ms/query (1M vectors)', 'Notes': 'Graph-based, high memory'},\n])\n\nprint('\\n' + '=' * 80)\nprint('TABLE 4.3: FAISS Index Types — Performance Reference')\nprint('=' * 80)\nprint(faiss_lit.to_string(index=False))\n\n# Angular margin loss comparison\nprint('\\n' + '=' * 80)\nprint('TABLE 4.4: Angular Margin Loss Functions — Evolution')\nprint('=' * 80)\nmargin_df = pd.DataFrame([\n    {'Loss': 'Softmax', 'Formula': 'log(e^s / sum(e^s))', 'LFW%': '~97.0', 'Key_Innovation': 'Baseline'},\n    {'Loss': 'SphereFace (A-Softmax)', 'Formula': 'cos(m*theta)', 'LFW%': '99.42', 'Key_Innovation': 'Multiplicative angular margin'},\n    {'Loss': 'CosFace (LMCL)', 'Formula': 'cos(theta) - m', 'LFW%': '99.73', 'Key_Innovation': 'Additive cosine margin'},\n    {'Loss': 'ArcFace', 'Formula': 'cos(theta + m)', 'LFW%': '99.83', 'Key_Innovation': 'Additive angular margin (geodesic)'},\n])\nprint(margin_df.to_string(index=False))\n\n# Save all tables\nrecognition_lit.to_csv(f'{RESULTS_DIR}/literature_face_recognition.csv', index=False)\ndetection_lit.to_csv(f'{RESULTS_DIR}/literature_face_detection.csv', index=False)\nprint(f'\\nLiterature tables saved to: {RESULTS_DIR}')\nprint('Note: \"This work\" rows will be filled after running evaluation sections above.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(f\"\\nPhase 4 results saved to: {RESULTS_DIR}\")\nprint(\"Sections completed:\")\nprint(\"  4.1: Face detection with RetinaFace\")\nprint(\"  4.2: ArcFace embedding extraction (512-d)\")\nprint(\"  4.3: FAISS similarity search\")\nprint(\"  4.4: Recognition threshold evaluation (FAR/FRR)\")\nprint(\"  4.5: Robustness under outdoor conditions\")\nprint(\"  4.6: Literature comparison with published benchmarks\")\nprint(\"Next: Open Phase5_Model_Optimization.ipynb\")",
   "execution_count": null,
   "outputs": []
  }
 ]
}