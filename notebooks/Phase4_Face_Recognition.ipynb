{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Face Detection & Recognition Evaluation\n",
    "Using RetinaFace for detection, ArcFace for feature extraction, and FAISS for matching.\n",
    "\n",
    "**Metrics**: Recognition Accuracy, FAR, FRR, Latency\n",
    "**Datasets**: LFW, WiderFace (preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "PROJECT_DIR = '/content/drive/MyDrive/computer_vision'\n",
    "DATASETS_DIR = f'{PROJECT_DIR}/datasets'\n",
    "RESULTS_DIR = f'{PROJECT_DIR}/results/phase4'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "%cd /content\n",
    "!rm -rf computer_vision_expirement\n",
    "!git clone https://github.com/Ib-Programmer/computer_vision_expirement.git\n",
    "%cd computer_vision_expirement\n",
    "\n",
    "!pip install -q insightface onnxruntime-gpu faiss-gpu\n",
    "!pip install -q -r requirements.txt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Face Detection with RetinaFace"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# Initialize face analysis (RetinaFace + ArcFace)\n",
    "app = FaceAnalysis(name='buffalo_l', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "print(\"InsightFace loaded: RetinaFace (detection) + ArcFace (recognition)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import glob\n",
    "import time\n",
    "\n",
    "lfw_test = sorted(glob.glob(f'{DATASETS_DIR}/lfw_processed/test/*.jpg'))\n",
    "print(f\"LFW test images: {len(lfw_test)}\")\n",
    "\n",
    "detection_results = []\n",
    "detection_times = []\n",
    "\n",
    "for img_path in lfw_test[:200]:  # Process 200 images\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        continue\n",
    "    \n",
    "    start = time.time()\n",
    "    faces = app.get(img)\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "    \n",
    "    detection_times.append(elapsed)\n",
    "    detection_results.append({\n",
    "        'path': img_path,\n",
    "        'num_faces': len(faces),\n",
    "        'time_ms': elapsed\n",
    "    })\n",
    "\n",
    "print(f\"Processed: {len(detection_results)} images\")\n",
    "print(f\"Avg detection time: {np.mean(detection_times):.1f} ms\")\n",
    "print(f\"Faces found: {sum(r['num_faces'] for r in detection_results)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feature Extraction (ArcFace Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extract face embeddings from detected faces\n",
    "embeddings_db = []\n",
    "labels_db = []\n",
    "\n",
    "print(\"Extracting face embeddings from LFW...\")\n",
    "for img_path in lfw_test[:200]:\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        continue\n",
    "    \n",
    "    faces = app.get(img)\n",
    "    for face in faces:\n",
    "        embedding = face.embedding  # 512-d ArcFace embedding\n",
    "        label = os.path.basename(os.path.dirname(img_path)) if '/' in img_path else os.path.basename(img_path)\n",
    "        \n",
    "        embeddings_db.append(embedding)\n",
    "        labels_db.append(label)\n",
    "\n",
    "embeddings_db = np.array(embeddings_db).astype('float32')\n",
    "print(f\"Extracted {len(embeddings_db)} embeddings, shape: {embeddings_db.shape}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Face Matching with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import faiss\n",
    "\n",
    "if len(embeddings_db) > 0:\n",
    "    # Normalize embeddings for cosine similarity\n",
    "    faiss.normalize_L2(embeddings_db)\n",
    "    \n",
    "    # Build FAISS index\n",
    "    dimension = embeddings_db.shape[1]  # 512\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner product (cosine after normalization)\n",
    "    index.add(embeddings_db)\n",
    "    \n",
    "    print(f\"FAISS index built: {index.ntotal} vectors, {dimension}-d\")\n",
    "    \n",
    "    # Search: query each embedding against the database\n",
    "    k = 5  # top-5 matches\n",
    "    search_start = time.time()\n",
    "    distances, indices = index.search(embeddings_db[:50], k)  # Query first 50\n",
    "    search_time = (time.time() - search_start) * 1000\n",
    "    \n",
    "    print(f\"Search time for 50 queries: {search_time:.1f} ms\")\n",
    "    print(f\"Avg per query: {search_time/50:.2f} ms\")\n",
    "    \n",
    "    # Show sample results\n",
    "    print(\"\\nSample matches (query -> top match, similarity):\")\n",
    "    for i in range(min(5, len(distances))):\n",
    "        print(f\"  Query {i}: match_idx={indices[i][1]}, similarity={distances[i][1]:.4f}\")\n",
    "else:\n",
    "    print(\"No embeddings extracted. Check face detection results.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Evaluate Recognition Performance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_recognition(embeddings, labels, thresholds=np.arange(0.1, 1.0, 0.05)):\n",
    "    \"\"\"Evaluate face recognition at different thresholds.\"\"\"\n",
    "    faiss.normalize_L2(embeddings.copy())\n",
    "    \n",
    "    results = []\n",
    "    n = len(embeddings)\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "        \n",
    "        for i in range(min(n, 100)):  # Sample pairs\n",
    "            for j in range(i+1, min(n, 100)):\n",
    "                sim = np.dot(embeddings[i], embeddings[j])\n",
    "                same_person = (labels[i] == labels[j])\n",
    "                \n",
    "                if sim >= threshold:\n",
    "                    if same_person:\n",
    "                        tp += 1\n",
    "                    else:\n",
    "                        fp += 1\n",
    "                else:\n",
    "                    if same_person:\n",
    "                        fn += 1\n",
    "                    else:\n",
    "                        tn += 1\n",
    "        \n",
    "        total_genuine = tp + fn\n",
    "        total_impostor = fp + tn\n",
    "        \n",
    "        far = fp / max(total_impostor, 1)\n",
    "        frr = fn / max(total_genuine, 1)\n",
    "        accuracy = (tp + tn) / max(tp + fp + tn + fn, 1)\n",
    "        \n",
    "        results.append({\n",
    "            'Threshold': round(threshold, 2),\n",
    "            'Accuracy': round(accuracy, 4),\n",
    "            'FAR': round(far, 4),\n",
    "            'FRR': round(frr, 4),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "if len(embeddings_db) > 0:\n",
    "    eval_df = evaluate_recognition(embeddings_db, labels_db)\n",
    "    print(\"Recognition Performance at Different Thresholds:\")\n",
    "    print(eval_df.to_string(index=False))\n",
    "    eval_df.to_csv(f'{RESULTS_DIR}/recognition_metrics.csv', index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Test Robustness Under Outdoor Conditions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conditions = ['fog', 'low_light', 'motion_blur', 'rain']\n",
    "condition_results = []\n",
    "\n",
    "aug_base = f'{PROJECT_DIR}/outputs/augmented/lfw/train'\n",
    "\n",
    "for condition in conditions:\n",
    "    aug_dir = f'{aug_base}/{condition}'\n",
    "    if not os.path.exists(aug_dir):\n",
    "        print(f\"[SKIP] {condition}: augmented data not found\")\n",
    "        continue\n",
    "    \n",
    "    aug_images = sorted(glob.glob(f'{aug_dir}/*.jpg'))[:50]\n",
    "    detected = 0\n",
    "    total = 0\n",
    "    \n",
    "    for img_path in aug_images:\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        total += 1\n",
    "        faces = app.get(img)\n",
    "        if len(faces) > 0:\n",
    "            detected += 1\n",
    "    \n",
    "    rate = detected / max(total, 1) * 100\n",
    "    condition_results.append({'Condition': condition, 'Images': total, 'Detected': detected, 'Rate': f'{rate:.1f}%'})\n",
    "    print(f\"  {condition}: {detected}/{total} faces detected ({rate:.1f}%)\")\n",
    "\n",
    "if condition_results:\n",
    "    cond_df = pd.DataFrame(condition_results)\n",
    "    print(\"\\nRobustness Summary:\")\n",
    "    print(cond_df.to_string(index=False))\n",
    "    cond_df.to_csv(f'{RESULTS_DIR}/robustness_analysis.csv', index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"\\nPhase 4 results saved to: {RESULTS_DIR}\")\n",
    "print(\"Next: Open Phase5_Model_Optimization.ipynb\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}